{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano, theano.tensor as T\n",
    "from theano.tensor import constant as tconstant\n",
    "import numpy as np\n",
    "from theano import shared\n",
    "from theano.ifelse import ifelse\n",
    "from theano.compile.debugmode import DebugMode\n",
    "from collections import OrderedDict\n",
    "srng = theano.tensor.shared_randomstreams.RandomStreams(1234)\n",
    "np_rng = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    Base object for neural network layers.\n",
    "\n",
    "    A layer has an input set of neurons, and\n",
    "    a hidden activation. The activation, f, is a\n",
    "    function applied to the affine transformation\n",
    "    of x by the connection matrix W, and the bias\n",
    "    vector b.\n",
    "\n",
    "    > y = f ( W * x + b )\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, activation, clip_gradients=False):\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation  = activation\n",
    "        self.clip_gradients = clip_gradients\n",
    "        self.is_recursive = False\n",
    "        self.create_variables()\n",
    "\n",
    "    def create_variables(self):\n",
    "        \"\"\"\n",
    "        Create the connection matrix and the bias vector\n",
    "        \"\"\"\n",
    "        self.linear_matrix        = create_shared(self.hidden_size, self.input_size, name=\"Layer.linear_matrix\")\n",
    "        self.bias_matrix          = create_shared(self.hidden_size, name=\"Layer.bias_matrix\")\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"\n",
    "        The hidden activation of the network\n",
    "        \"\"\"\n",
    "        if self.clip_gradients is not False:\n",
    "            x = clip_gradient(x, self.clip_gradients)\n",
    "\n",
    "        if x.ndim > 1:\n",
    "            return self.activation(\n",
    "                T.dot(self.linear_matrix, x.T) + self.bias_matrix[:,None] ).T\n",
    "        else:\n",
    "            return self.activation(\n",
    "                T.dot(self.linear_matrix, x) + self.bias_matrix )\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.linear_matrix, self.bias_matrix]\n",
    "\n",
    "    @params.setter\n",
    "    def params(self, param_list):\n",
    "        self.linear_matrix.set_value(param_list[0].get_value())\n",
    "        self.bias_matrix.set_value(param_list[1].get_value())\n",
    "        \n",
    "class MDLSTMLayer(Layer):\n",
    "    \"\"\"Multi-dimensional long short-term memory cell layer.\n",
    "\n",
    "    The cell-states are explicitly passed on through a part of\n",
    "    the input/output buffers (which should be connected correctly with IdentityConnections).\n",
    "\n",
    "    The input consists of 4 parts, in the following order:\n",
    "    - input gate\n",
    "    - forget gates (1 per dim)\n",
    "    - cell input\n",
    "    - output gate\n",
    "    - previous states (1 per dim)\n",
    "\n",
    "    The output consists of two parts:\n",
    "    - cell output\n",
    "    - current statte\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, shape, activation=T.tanh, clip_gradients=False):\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation  = activation\n",
    "        self.clip_gradients = clip_gradients\n",
    "        self.is_recursive = True\n",
    "        self.create_variables()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def create_variables(self):\n",
    "        \"\"\"\n",
    "        Create the different LSTM gates and\n",
    "        their variables, along with the initial\n",
    "        hidden state for the memory cells and\n",
    "        the initial hidden activation.\n",
    "\n",
    "        \"\"\"\n",
    "        # input gate for cells\n",
    "        self.in_gate_layer     = Layer(self.input_size + 2*self.hidden_size, self.hidden_size, T.nnet.sigmoid, self.clip_gradients)\n",
    "        # forget gate for cells\n",
    "        self.forget_gate_x_layer = Layer(self.input_size + 2*self.hidden_size, self.hidden_size, T.nnet.sigmoid, self.clip_gradients)\n",
    "        self.forget_gate_y_layer = Layer(self.input_size + 2*self.hidden_size, self.hidden_size, T.nnet.sigmoid, self.clip_gradients)\n",
    "        # input modulation for cells\n",
    "        self.in_cell_layer    = Layer(self.input_size + 2*self.hidden_size, self.hidden_size, T.tanh, self.clip_gradients)\n",
    "        # output modulation\n",
    "        self.out_gate_layer    = Layer(self.input_size + 2*self.hidden_size, self.hidden_size, T.nnet.sigmoid, self.clip_gradients)\n",
    "\n",
    "        # keep these layers organized\n",
    "        self.internal_layers = [self.in_gate_layer, self.forget_gate_x_layer, self.forget_gate_y_layer, self.in_cell_layer, self.out_gate_layer]\n",
    "\n",
    "        # store the memory cells in first n spots, and store the current\n",
    "        # output in the next n spots:\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters given by the 4 gates and the\n",
    "        initial hidden activation of this LSTM cell\n",
    "        layer.\n",
    "        \"\"\"\n",
    "        return [param for layer in self.internal_layers for param in layer.params]\n",
    "    @params.setter\n",
    "    def params(self, param_list):\n",
    "        start = 0\n",
    "        for layer in self.internal_layers:\n",
    "            end = start + len(layer.params)\n",
    "            layer.params = param_list[start:end]\n",
    "            start = end\n",
    "    def postprocess_activation(self, x, *args):\n",
    "        if x.ndim > 1:\n",
    "            return x[:, self.hidden_size:]\n",
    "        else:\n",
    "            return x[self.hidden_size:]\n",
    "\n",
    "\n",
    "        \n",
    "    def create_prediction(self, x_input):\n",
    "        directions = tconstant(np.array([[-1,-1],[-1,1],[1,-1],[1,1]]))\n",
    "        res,update = theano.scan(lambda direction, xipt: self.create_prediction_once(xipt, direction),\n",
    "                    sequences = [directions],\n",
    "                    outputs_info = None,\n",
    "                    non_sequences = [x_input])\n",
    "        return T.reshape(res,(4*self.hidden_size, self.shape[0], self.shape[1]))\n",
    "    def create_prediction_once(self, x_input, direction):\n",
    "        ''' input: \n",
    "                x_input: a tensor of size H*W*C\n",
    "                shape: spatial dimension of x_input and x_output: (H,W)\n",
    "                direction:(-1,-1): from top left\n",
    "                          (-1, 1): from top right\n",
    "                          ( 1,-1): from bottom left\n",
    "                          ( 1, 1): from bottom right                            \n",
    "            output:\n",
    "                x_output: a tensor of size H*W*K\n",
    "        '''\n",
    "        #   e.g. for shape of (3,3), the scanning order of LSTM would be [0,1,2,3,4,5,6,7,8]\n",
    "        scan_order = self.permsForSwiping(direction)\n",
    "        \n",
    "        # store previous cell states and hidden activations\n",
    "#         hidden_mat = T.zeros([self.hidden_size, self.shape[0], self.shape[1]], dtype=theano.config.floatX)\n",
    "#         cell_mat = T.zeros([self.hidden_size, self.shape[0], self.shape[1]], dtype=theano.config.floatX)\n",
    "            \n",
    "        CH = T.zeros([self.shape[0], 2*self.hidden_size])       \n",
    "        \n",
    "        xyv = tconstant(np.array([np.meshgrid(range(self.shape[0]),range(self.shape[1]))]))\n",
    "        xyv_seq = T.transpose(T.reshape(xyv,(2,self.shape[0]*self.shape[1])))\n",
    "        cell_order = xyv_seq[scan_order,:]\n",
    "        \n",
    "        # flatten input\n",
    "        X_input_reshaped = T.transpose(T.reshape(T.transpose(x_input,[0,2,1]), (self.input_size,self.shape[0]*self.shape[1])))\n",
    "        X_input_reshaped = X_input_reshaped[scan_order,:]\n",
    "        \n",
    "        res, updates = theano.scan(lambda x_inpt, cell_loc, ch_x, ch_y, dire:\n",
    "                                        self.activate(x_inpt,T.concatenate([\n",
    "                                        ifelse((T.lt(cell_loc[0]+dire[0],shared(0)) | T.ge(cell_loc[0]+dire[0],self.shape[0])),\n",
    "                                        T.zeros(2*self.hidden_size),ch_x),\n",
    "                                        ifelse((T.lt(cell_loc[1]+dire[1],shared(0)) | T.ge(cell_loc[1]+dire[1],self.shape[1])),\n",
    "                                        T.zeros(2*self.hidden_size),ch_y)])),\n",
    "                                        sequences = [X_input_reshaped,cell_order],\n",
    "                                       \n",
    "                                        outputs_info=[dict(initial=CH, taps=[-1, -self.shape[0]])],\n",
    "                                        non_sequences = direction)\n",
    "        hidden_mat = T.reshape(res[:,self.hidden_size:],(self.hidden_size,self.shape[0],self.shape[1]))\n",
    "        return hidden_mat\n",
    "  \n",
    "    def permsForSwiping(self, direction):\n",
    "        \"\"\" Given the spatial dimension of the input\n",
    "            Return the correct permutations of blocks for all swiping direction.\n",
    "        \"\"\"\n",
    "        identity = T.arange(T.prod(self.shape))    \n",
    "        identity_flipped = T.reshape(identity,self.shape,ndim=2)[::-1,:].flatten()\n",
    "\n",
    "        perms = ifelse(T.all(T.eq(direction,T.constant([-1,-1]))), identity,\n",
    "                         ifelse(T.all(T.eq(direction,T.constant([1,1]))),identity[::-1],\n",
    "                         ifelse(T.all(T.eq(direction,T.constant([-1,1]))),identity_flipped,identity_flipped[::-1])))\n",
    "        return perms\n",
    "    def activate(self, x, h):\n",
    "        \"\"\"\n",
    "        The hidden activation, h, of the network, along\n",
    "        with the new values for the memory cells, c,\n",
    "        Both are concatenated as follows:\n",
    "\n",
    "        >      y = f( x, past )\n",
    "\n",
    "        Or more visibly, with past = [prev_c, prev_h]\n",
    "\n",
    "        > [c, h] = f( x, [prev_c, prev_h] )\n",
    "        \n",
    "        Currently we don't have peephole connections.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if h.ndim > 1:\n",
    "            prev_c_x = h[:, :self.hidden_size]\n",
    "            #previous activations of the hidden layer in x direction\n",
    "            prev_h_x = h[:, self.hidden_size:self.hidden_size+self.hidden_size]\n",
    "            #previous memory cell values in y direction\n",
    "            prev_c_y = h[:, self.hidden_size+self.hidden_size:2*self.hidden_size+self.hidden_size]\n",
    "            #previous activations of the hidden layer in x direction\n",
    "            prev_h_y = h[:, 2*self.hidden_size+self.hidden_size:]\n",
    "        else:\n",
    "            prev_c_x = h[:self.hidden_size]\n",
    "            #previous activations of the hidden layer in x direction\n",
    "            prev_h_x = h[self.hidden_size:2*self.hidden_size]\n",
    "            #previous memory cell values in y direction\n",
    "            prev_c_y = h[2*self.hidden_size:3*self.hidden_size]\n",
    "            #previous activations of the hidden layer in x direction\n",
    "            prev_h_y = h[3*self.hidden_size:]\n",
    "            \n",
    "        # input and previous hidden states in two directions constitute the actual\n",
    "        # input to the LSTM:\n",
    "        if h.ndim > 1:\n",
    "            obs = T.concatenate([x, prev_h_x, prev_h_y], axis=1)\n",
    "        else:\n",
    "            obs = T.concatenate([x, prev_h_x, prev_h_y], axis=0)\n",
    "            \n",
    "        # input gate\n",
    "        in_gate = self.in_gate_layer.activate(obs)\n",
    "        # forget (in two directions)\n",
    "        forget_gate_x = self.forget_gate_x_layer.activate(obs)\n",
    "        forget_gate_y = self.forget_gate_y_layer.activate(obs)\n",
    "        \n",
    "        # compute cell input \n",
    "        in_cell = self.in_cell_layer.activate(obs)\n",
    "        # new memory cells\n",
    "        next_c = forget_gate_x * prev_c_x + forget_gate_y * prev_c_y +in_cell * in_gate\n",
    "        \n",
    "        # output gate\n",
    "        out_gate = self.out_gate_layer.activate(obs)\n",
    "        # new hidden states\n",
    "        next_h = out_gate * T.tanh(next_c)     \n",
    "        if h.ndim > 1:\n",
    "            return T.concatenate([next_c, next_h], axis=1)\n",
    "        else:\n",
    "            return T.concatenate([next_c, next_h], axis=0)\n",
    "        \n",
    "def create_shared(hidden_size, in_size=None, name=None):\n",
    "    \"\"\"\n",
    "    Creates a shared matrix or vector\n",
    "    using the given in_size and hidden_size.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "\n",
    "    hidden_size int            : outer dimension of the\n",
    "                              vector or matrix\n",
    "    in_size  int (optional) : for a matrix, the inner\n",
    "                              dimension.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "\n",
    "    theano shared : the shared matrix, with random numbers in it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if in_size is None:\n",
    "        return theano.shared(random_initialization((hidden_size, )), name=name)\n",
    "    else:\n",
    "        return theano.shared(random_initialization((hidden_size, in_size)), name=name)\n",
    "    \n",
    "def random_initialization(size):\n",
    "    return (np_rng.standard_normal(size) * 1. / size[0]).astype(theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(3,4,6)\n",
    "net = MDLSTMLayer(3, 2,(4,6))  \n",
    "theano.config.compute_test_value = 'warn'\n",
    "img = T.tensor3()\n",
    "img.tag.test_value =  x\n",
    "direction = T.vector()\n",
    "direction.tag.test_value = [-1,-1]\n",
    "f = theano.function(inputs=[img], outputs=[net.create_prediction(img)])\n",
    "g = theano.function(inputs=[img,direction], outputs=[net.create_prediction_once(img,direction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.09133504 -0.01384627  0.13693459 -0.01521263  0.18107321 -0.00380177]\n",
      "  [ 0.26117384 -0.0066828   0.13086979 -0.04064852  0.2067028  -0.04933781]\n",
      "  [ 0.23990984 -0.04504092  0.27120143 -0.06429556  0.24720783 -0.05795617]\n",
      "  [ 0.50238457 -0.10902152  0.43283703 -0.12133762  0.49449936 -0.13653737]]\n",
      "\n",
      " [[ 0.39359728 -0.08791297  0.53469274 -0.17976823  0.43371281 -0.24976274]\n",
      "  [ 0.70525102 -0.31407452  0.41415621 -0.10011591  0.33159066 -0.23904572]\n",
      "  [ 0.5999414  -0.3490413   0.67881947 -0.40292604  0.38532258 -0.10804207]\n",
      "  [ 0.27466529 -0.23058987  0.44999066 -0.4540891   0.64843316 -0.48363151]]] \n",
      "[[[ 0.09133504 -0.01384627  0.13693459 -0.01521263  0.18107321 -0.00380177]\n",
      "  [ 0.26117384 -0.0066828   0.13086979 -0.04064852  0.2067028  -0.04933781]\n",
      "  [ 0.23990984 -0.04504092  0.27120143 -0.06429556  0.24720783 -0.05795617]\n",
      "  [ 0.50238457 -0.10902152  0.43283703 -0.12133762  0.49449936 -0.13653737]]\n",
      "\n",
      " [[ 0.39359728 -0.08791297  0.53469274 -0.17976823  0.43371281 -0.24976274]\n",
      "  [ 0.70525102 -0.31407452  0.41415621 -0.10011591  0.33159066 -0.23904572]\n",
      "  [ 0.5999414  -0.3490413   0.67881947 -0.40292604  0.38532258 -0.10804207]\n",
      "  [ 0.27466529 -0.23058987  0.44999066 -0.4540891   0.64843316 -0.48363151]]]\n"
     ]
    }
   ],
   "source": [
    "[res1] = g(x,[-1,-1])\n",
    "[res2] = f(x)\n",
    "print res1,'\\n',res2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
